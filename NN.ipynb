{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network from the Ground-Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init by importing `numpy` (this is mainly used) and `matplotlib` (only used to show the convergence). \n",
    "Note we make use of some `@staticmethods` which have been grouped by there activation function (e.g. Sigmoid, TANH), this way it allows for modular implementation of the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # required for most computations\n",
    "import json         # Used to return the model params\n",
    "import matplotlib.pyplot as plt # Used to display the training loss / itterations\n",
    "import random       # Used to randomly move through the training set\n",
    "\n",
    "class SigmoidActivation:\n",
    "    @staticmethod\n",
    "    def forward(input) -> np.array:\n",
    "        ex = np.exp(input)\n",
    "        return ex / (ex + 1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(input) -> np.array:\n",
    "        sigmoid = SigmoidActivation.forward(input)\n",
    "        return sigmoid * (1 - sigmoid)\n",
    "    \n",
    "class TanhActivation:\n",
    "    @staticmethod\n",
    "    def forward(input) -> np.array:\n",
    "        ex = np.exp(input)\n",
    "        ex_min = np.exp(-1*input)\n",
    "        return 1 / (1 + np.exp(input))\n",
    "        # return (ex - ex_min) / (ex + ex_min)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(input) -> np.array:\n",
    "        return None # not implemented yet\n",
    "    \n",
    "class ReLU:\n",
    "    @staticmethod\n",
    "    def forward(input) -> np.array:\n",
    "        return np.maximum(0, input)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(input) -> np.array:\n",
    "        return np.where(input <= 0, 0, 1)\n",
    "\n",
    "def softmax(x: np.array) -> np.array:\n",
    "    return np.exp(x)/sum(np.exp(x))\n",
    "\n",
    "def calculateLoss(y_pred, y_true):\n",
    "    loss = (y_pred - y_true) ** 2\n",
    "    return np.mean(loss)\n",
    "\n",
    "# This function is used to check the activation function selected for the NN (if exists and if yes return the right object)\n",
    "def checkActivation(activation_func) -> object:\n",
    "    if activation_func == \"sigmoid\":\n",
    "            activation = SigmoidActivation\n",
    "    elif activation_func == \"tanh\":\n",
    "        activation = TanhActivation\n",
    "    elif activation_func == \"relu\":\n",
    "        activation = ReLU\n",
    "    else:\n",
    "        raise AttributeError(\"Chosen activation function does not exist.\")\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the `layer` class which is the most simple form of the Neural Network. In this case it has been chosen to vectorise each layer. So each node is a position an `ndarray` where the value is the weight of the node.\n",
    "\n",
    "Furthermore, the `getLayer` method has been implemented to return the weights of the layer (getter func) and the `compute` method does the operations to calculate the output of the layer given an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer:\n",
    "    def __init__(self, numberOfOutput: int, numberOfNodes: int):\n",
    "        self.weigths = np.random.rand(numberOfNodes)\n",
    "        self.bias = np.random.rand(numberOfOutput, numberOfNodes)\n",
    "\n",
    "    def getLayer(self):\n",
    "        return(self.weigths)\n",
    "    \n",
    "    def compute(self, inputs: np.array, activation: object) -> np.array:\n",
    "        print(\"ITTERATION!\")\n",
    "        print(f\"inputs: {inputs}\")\n",
    "        print(f\"weights: {self.weigths}\")\n",
    "        print(f\"bias: {self.bias}\")\n",
    "        W = np.dot(inputs, self.weigths) + self.bias\n",
    "        return  activation.forward(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the `FFN` class (Feed Forward Network) has been defined. This is a rather complex class so I will break it down method by method. In general the class represents a simple neural nerwork as an `object`. This object holds all layers (which in turn hold als weights and biases) of the network AND the methods defined in the class can be used to do operations on the network.\n",
    "\n",
    "1. **init method** \n",
    "\n",
    "The init method is used to initialise a network. This is done by passing the dimentions of network where the position of the array corrosponds to a layer and the value corrosponds to the number of neurons in each layer (e.g. [3, 3, 1] -> 3 layers, where; layer 1 = 3 neurons; layer 2 = 3 neurons; layer 3 = 1 neuron). \n",
    "Additionally, activation function can be chosen (by default sigmoid) and an learning rate (alpha). \n",
    "\n",
    "2. **forward method**\n",
    "\n",
    "The forward method does a forward pass through the network given a valid input. This can be seen as the `.predict` method in the tensorflow lib. \n",
    "\n",
    "3. **backward method**\n",
    "\n",
    "The backward method does backprogagation of the network. \n",
    "The following algorithm has been implemented to tune the weights and biases. \n",
    "\n",
    "4. **train method**\n",
    "\n",
    "The train method is a wrapper of the backward method with some additional features. It uses the `backward method` to train the model in a _for-loop_ so that the model can be easly trained multiple itterations and an implementation which takes a set number of samples of the loss and prediction which is than displayed in a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN:\n",
    "    def __init__(self, dimensions: list[int], activation=\"sigmoid\", alpha=1) -> None:\n",
    "        if len(dimensions) < 2:\n",
    "            raise AssertionError(\"Network must have at least two layers (input and output).\")\n",
    "        \n",
    "        self.activation = checkActivation(activation_func=activation)\n",
    "        self.learningRate = alpha\n",
    "\n",
    "        self.layers = []\n",
    "        for i in range(0, len(dimensions)):\n",
    "            layer_dim = dimensions[i]\n",
    "            print(f\"i: {i}, len: {len(dimensions)}\")\n",
    "            if i != (len(dimensions) - 1): \n",
    "                output_dim = dimensions[i+1]\n",
    "            else: \n",
    "                output_dim = layer_dim\n",
    "            self.layers.append(layer(output_dim, layer_dim))\n",
    "\n",
    "    def displayModel(self) -> None:\n",
    "        json_obj = {\n",
    "            \"Number of Layers\" : len(self.layers),\n",
    "            \"Layers\" : []\n",
    "        }\n",
    "\n",
    "        i = 1\n",
    "        for layer in self.layers:\n",
    "            layer_data = {\n",
    "                \"Layer\" : i,\n",
    "                \"Number of Nodes\" : len(layer.weigths),\n",
    "                \"Number Biases\" : len(layer.bias),\n",
    "                \"Weights\" : layer.weigths.tolist(),\n",
    "                \"Bias\" : layer.bias.tolist()\n",
    "            }\n",
    "            i += 1\n",
    "            json_obj[\"Layers\"].append(layer_data)\n",
    "\n",
    "        print(json.dumps(json_obj, indent=4))\n",
    "\n",
    "    def forward(self, input: list[float]) -> list[float]:\n",
    "        activation = input\n",
    "        for layer in self.layers:\n",
    "            activation = layer.compute(activation, self.activation)\n",
    "        return activation\n",
    "        \n",
    "    def backward(self, input: list[float], y_true: list[float]) -> None:\n",
    "        store_impulse = [input]\n",
    "        current_output = input\n",
    "\n",
    "        for layer in self.layers:\n",
    "            current_output = layer.compute(current_output, activation=self.activation)\n",
    "            store_impulse.append(np.array(current_output)) \n",
    "\n",
    "        y_pred = store_impulse[-1]\n",
    "        error = y_pred - y_true\n",
    "        delta = error * self.activation.backward(y_pred)\n",
    "\n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            layer = self.layers[i]\n",
    "            activation_layer = store_impulse[i]\n",
    "\n",
    "            activation_layer = np.array(activation_layer).reshape(1, -1)\n",
    "            delta = delta.reshape(1, -1)\n",
    "\n",
    "\n",
    "            layer.weigths -= self.learningRate * np.dot(activation_layer.T, delta)\n",
    "            layer.bias -= self.learningRate * np.sum(delta, axis=0)\n",
    "\n",
    "            if i > 0:\n",
    "                delta = np.dot(delta, self.layers[i].weigths.T) * self.activation.backward(store_impulse[i])\n",
    "\n",
    "    def train(self, X: list[list[float]], y: list[list[float]], max_itterations=1000, plot_loss=True) -> None:\n",
    "        if len(X) != len(y):\n",
    "            raise ValueError(\"The length of X and y must be the same.\")\n",
    "        losses = []\n",
    "\n",
    "        for i in range(max_itterations):\n",
    "            iteration_loss = 0 \n",
    "            for _ in range(len(X)): \n",
    "                index = random.randint(0, len(X) - 1)\n",
    "                xSelected = X[index]\n",
    "                ySelected = y[index]\n",
    "\n",
    "                self.backward(xSelected, ySelected)\n",
    "\n",
    "                if plot_loss:\n",
    "                    yPredicted = self.forward(xSelected)\n",
    "                    example_loss = np.mean(yPredicted - ySelected)\n",
    "                    iteration_loss += example_loss\n",
    "\n",
    "            if plot_loss:\n",
    "                avg_loss = iteration_loss / len(X)\n",
    "                losses.append(avg_loss)\n",
    "\n",
    "        if plot_loss:\n",
    "            plt.plot(range(max_itterations), losses)\n",
    "            plt.xlabel(\"Iterations\")\n",
    "            plt.ylabel(\"Average Loss\")\n",
    "            plt.title(\"Training Loss over Iterations\")\n",
    "            plt.grid()\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0, len: 3\n",
      "i: 1, len: 3\n",
      "i: 2, len: 3\n",
      "ITTERATION!\n",
      "inputs: [0, 0, 0, 0, 0, 0, 1, 0]\n",
      "weights: [0.82849596 0.88488761 0.43816297 0.33149125 0.38319134 0.67650209\n",
      " 0.34639775 0.8063838 ]\n",
      "bias: [[0.47263886 0.97369162 0.97574955 0.04138936 0.07060159 0.63200443\n",
      "  0.36404243 0.68539744]\n",
      " [0.76212882 0.91241896 0.39532338 0.03322995 0.78945612 0.61962017\n",
      "  0.31963175 0.09895953]\n",
      " [0.2381396  0.1362166  0.68955988 0.28081035 0.03191991 0.42903582\n",
      "  0.07331574 0.94669599]]\n",
      "ITTERATION!\n",
      "inputs: [[0.6940318  0.78919657 0.78953874 0.59574988 0.60276499 0.72679106\n",
      "  0.67049842 0.73726378]\n",
      " [0.75185432 0.77882234 0.67737211 0.5937833  0.75691759 0.72432507\n",
      "  0.66061353 0.60953482]\n",
      " [0.64211078 0.61836502 0.73806927 0.65185614 0.59346728 0.68469511\n",
      "  0.60341469 0.78467038]]\n",
      "weights: [0.23874636 0.33980828 0.99640392]\n",
      "bias: [[0.75852929 0.32526029 0.95456396]\n",
      " [0.77126167 0.98937168 0.70527436]\n",
      " [0.72203406 0.65437884 0.88263147]\n",
      " [0.9998074  0.40444162 0.5080641 ]\n",
      " [0.10307328 0.54908688 0.6057169 ]\n",
      " [0.00870394 0.01573307 0.10926118]\n",
      " [0.0770253  0.2583552  0.31042246]\n",
      " [0.13846244 0.79581058 0.75901014]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (3,8) and (3,) not aligned: 8 (dim 1) != 3 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 21\u001b[0m\n\u001b[1;32m     10\u001b[0m yTrain \u001b[38;5;241m=\u001b[39m[[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     11\u001b[0m         [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m], \n\u001b[1;32m     12\u001b[0m         [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m], \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m         [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m], \n\u001b[1;32m     17\u001b[0m         [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m]]\n\u001b[1;32m     20\u001b[0m Network_ \u001b[38;5;241m=\u001b[39m FFN([\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m8\u001b[39m], alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mNetwork_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxTrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43myTrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_itterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[90], line 88\u001b[0m, in \u001b[0;36mFFN.train\u001b[0;34m(self, X, y, max_itterations, plot_loss)\u001b[0m\n\u001b[1;32m     85\u001b[0m xSelected \u001b[38;5;241m=\u001b[39m X[index]\n\u001b[1;32m     86\u001b[0m ySelected \u001b[38;5;241m=\u001b[39m y[index]\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxSelected\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mySelected\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m plot_loss:\n\u001b[1;32m     91\u001b[0m     yPredicted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(xSelected)\n",
      "Cell \u001b[0;32mIn[90], line 55\u001b[0m, in \u001b[0;36mFFN.backward\u001b[0;34m(self, input, y_true)\u001b[0m\n\u001b[1;32m     52\u001b[0m current_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     current_output \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     store_impulse\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39marray(current_output)) \n\u001b[1;32m     58\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m store_impulse[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[0;32mIn[81], line 14\u001b[0m, in \u001b[0;36mlayer.compute\u001b[0;34m(self, inputs, activation)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweigths\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbias: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m W \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweigths\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m  activation\u001b[38;5;241m.\u001b[39mforward(W)\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (3,8) and (3,) not aligned: 8 (dim 1) != 3 (dim 0)"
     ]
    }
   ],
   "source": [
    "xTrain =[[1, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 1, 0, 0, 0, 0, 0, 0], \n",
    "        [0, 0, 1, 0, 0, 0, 0, 0], \n",
    "        [0, 0, 0, 1, 0, 0, 0, 0], \n",
    "        [0, 0, 0, 0, 1, 0, 0, 0], \n",
    "        [0, 0, 0, 0, 0, 1, 0, 0], \n",
    "        [0, 0, 0, 0, 0, 0, 1, 0], \n",
    "        [0, 0, 0, 0, 0, 0, 0, 1]]\n",
    "\n",
    "yTrain =[[1, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 1, 0, 0, 0, 0, 0, 0], \n",
    "        [0, 0, 1, 0, 0, 0, 0, 0], \n",
    "        [0, 0, 0, 1, 0, 0, 0, 0], \n",
    "        [0, 0, 0, 0, 1, 0, 0, 0], \n",
    "        [0, 0, 0, 0, 0, 1, 0, 0], \n",
    "        [0, 0, 0, 0, 0, 0, 1, 0], \n",
    "        [0, 0, 0, 0, 0, 0, 0, 1]]\n",
    "\n",
    "\n",
    "Network_ = FFN([8, 3, 8], alpha=0.01)\n",
    "Network_.train(X=xTrain, y=yTrain, max_itterations=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.11143225 0.13147238 0.13277603 0.1342457  0.12013994 0.11962479\n",
      " 0.12160775 0.12671164]\n",
      "[0.12334278 0.12583952 0.12600368 0.126189   0.1244215  0.12435742\n",
      " 0.12460426 0.12524185]\n",
      "avg loss: 0.10705626228747613\n"
     ]
    }
   ],
   "source": [
    "xTest = [0, 0, 0, 1, 0, 0, 0, 0]\n",
    "yPred = Network_.forward(xTest)\n",
    "\n",
    "print(yPred)\n",
    "print(softmax(yPred)) # This should be equal to xTest\n",
    "print(f\"avg loss: {calculateLoss(yPred, xTest)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10289885 0.10289885 0.10289885 0.27970807 0.10289885 0.10289885\n",
      " 0.10289885 0.10289885]\n"
     ]
    }
   ],
   "source": [
    "print(softmax([0, 0, 0, 1, 0, 0, 0, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(yPred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Number of Layers\": 3,\n",
      "    \"Layers\": [\n",
      "        {\n",
      "            \"Layer\": 1,\n",
      "            \"Number of Nodes\": 8,\n",
      "            \"Number of Inputs\": 3,\n",
      "            \"Weights\": [\n",
      "                0.8284959564695161,\n",
      "                0.8848876102567131,\n",
      "                0.43816296842151303,\n",
      "                0.33149125119891176,\n",
      "                0.3831913357217924,\n",
      "                0.6765020893647472,\n",
      "                0.34639774775088794,\n",
      "                0.8063837990472243\n",
      "            ],\n",
      "            \"Bias\": [\n",
      "                [\n",
      "                    0.472638863176842,\n",
      "                    0.9736916200828568,\n",
      "                    0.9757495533260457,\n",
      "                    0.04138935669830235,\n",
      "                    0.07060158593021426,\n",
      "                    0.6320044286865178,\n",
      "                    0.364042428319279,\n",
      "                    0.6853974394631653\n",
      "                ],\n",
      "                [\n",
      "                    0.7621288199777693,\n",
      "                    0.9124189598463068,\n",
      "                    0.3953233825107837,\n",
      "                    0.03322994980444349,\n",
      "                    0.7894561171955026,\n",
      "                    0.6196201656792871,\n",
      "                    0.31963175298190616,\n",
      "                    0.09895953120572287\n",
      "                ],\n",
      "                [\n",
      "                    0.2381395982577279,\n",
      "                    0.13621659845716405,\n",
      "                    0.6895598757103635,\n",
      "                    0.28081034618939593,\n",
      "                    0.03191990509308584,\n",
      "                    0.4290358187582063,\n",
      "                    0.07331574085509773,\n",
      "                    0.9466959918189237\n",
      "                ]\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"Layer\": 2,\n",
      "            \"Number of Nodes\": 3,\n",
      "            \"Number of Inputs\": 8,\n",
      "            \"Weights\": [\n",
      "                0.23874636414908745,\n",
      "                0.33980827958029347,\n",
      "                0.9964039223145813\n",
      "            ],\n",
      "            \"Bias\": [\n",
      "                [\n",
      "                    0.7585292897517506,\n",
      "                    0.32526028504016835,\n",
      "                    0.954563957362111\n",
      "                ],\n",
      "                [\n",
      "                    0.7712616732425609,\n",
      "                    0.9893716825738534,\n",
      "                    0.7052743625985421\n",
      "                ],\n",
      "                [\n",
      "                    0.7220340557047779,\n",
      "                    0.6543788381995568,\n",
      "                    0.8826314733551824\n",
      "                ],\n",
      "                [\n",
      "                    0.9998073977151912,\n",
      "                    0.4044416170625753,\n",
      "                    0.5080640984472892\n",
      "                ],\n",
      "                [\n",
      "                    0.10307327914279107,\n",
      "                    0.5490868784089438,\n",
      "                    0.6057169039947506\n",
      "                ],\n",
      "                [\n",
      "                    0.00870394454511425,\n",
      "                    0.015733074032067806,\n",
      "                    0.109261184385401\n",
      "                ],\n",
      "                [\n",
      "                    0.07702530213134218,\n",
      "                    0.2583552025712812,\n",
      "                    0.3104224551421084\n",
      "                ],\n",
      "                [\n",
      "                    0.13846243839684602,\n",
      "                    0.7958105797381796,\n",
      "                    0.7590101413784037\n",
      "                ]\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"Layer\": 3,\n",
      "            \"Number of Nodes\": 8,\n",
      "            \"Number of Inputs\": 8,\n",
      "            \"Weights\": [\n",
      "                0.9069602205599535,\n",
      "                0.2735061081256731,\n",
      "                0.28831163891331024,\n",
      "                0.6351199341189215,\n",
      "                0.2672356736398682,\n",
      "                0.63363103306342,\n",
      "                0.5137196364314104,\n",
      "                0.7888375748238063\n",
      "            ],\n",
      "            \"Bias\": [\n",
      "                [\n",
      "                    0.3696590779959995,\n",
      "                    0.5587137987986959,\n",
      "                    0.3527558467910248,\n",
      "                    0.5589745718184639,\n",
      "                    0.45825768443440085,\n",
      "                    0.220591452183873,\n",
      "                    0.6725901104839689,\n",
      "                    0.9886493250923141\n",
      "                ],\n",
      "                [\n",
      "                    0.11301133068501523,\n",
      "                    0.06977003550564842,\n",
      "                    0.628594427263199,\n",
      "                    0.4821312368839207,\n",
      "                    0.588478169383191,\n",
      "                    0.2745006407919548,\n",
      "                    0.1062227129131631,\n",
      "                    0.06610511489475135\n",
      "                ],\n",
      "                [\n",
      "                    0.2735566969275901,\n",
      "                    0.902290366792749,\n",
      "                    0.860892304294693,\n",
      "                    0.9011556186981888,\n",
      "                    0.3220848385670314,\n",
      "                    0.08394193870522004,\n",
      "                    0.9627283964389767,\n",
      "                    0.36183820167642666\n",
      "                ],\n",
      "                [\n",
      "                    0.37115514821530016,\n",
      "                    0.33008826692146587,\n",
      "                    0.47750395400291534,\n",
      "                    0.9412283687822687,\n",
      "                    0.7959543700706762,\n",
      "                    0.20585475790703212,\n",
      "                    0.3487563660021459,\n",
      "                    0.8885279554642155\n",
      "                ],\n",
      "                [\n",
      "                    0.10875749256734213,\n",
      "                    0.3434311020357037,\n",
      "                    0.3080277657105861,\n",
      "                    0.5134187434929196,\n",
      "                    0.4180899641143496,\n",
      "                    0.47605427803437916,\n",
      "                    0.7078479257483581,\n",
      "                    0.37713387482573013\n",
      "                ],\n",
      "                [\n",
      "                    0.6049125883233416,\n",
      "                    0.841899335430292,\n",
      "                    0.6582351206524943,\n",
      "                    0.7919799982297836,\n",
      "                    0.04954309648224953,\n",
      "                    0.9724959057655331,\n",
      "                    0.7264345574653613,\n",
      "                    0.6308990814968681\n",
      "                ],\n",
      "                [\n",
      "                    0.28077423195028983,\n",
      "                    0.5318741701328603,\n",
      "                    0.34244536743487586,\n",
      "                    0.7497652258498226,\n",
      "                    0.7435053553440119,\n",
      "                    0.29713260790335105,\n",
      "                    0.8331948852270984,\n",
      "                    0.11488457457270873\n",
      "                ],\n",
      "                [\n",
      "                    0.9022315906448262,\n",
      "                    0.8360752569420538,\n",
      "                    0.9100538394586797,\n",
      "                    0.2666328063348943,\n",
      "                    0.21702917561074198,\n",
      "                    0.2171124186972625,\n",
      "                    0.6153990101387882,\n",
      "                    0.4821833019062701\n",
      "                ]\n",
      "            ]\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "Network_.displayModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
